# OmniRAG â€“ Multimodal Retrievalâ€‘Augmented Generation System

OmniRAG is a **pure Retrievalâ€‘Augmented Generation (RAG)** backend system that allows users to ingest documents (PDF, CSV) and ask questions that are answered **strictly from the ingested data**.

This project is intentionally designed to **avoid hallucinations** and behave like an **enterpriseâ€‘grade knowledge assistant**, not a generalâ€‘purpose chatbot.

---

## ğŸš€ Key Features

* ğŸ“„ **PDF Ingestion** â€“ Extracts and embeds document text
* ğŸ“Š **CSV Ingestion** â€“

  * Columnâ€‘level semantic embeddings
  * Rowâ€‘level embeddings (first 500 rows)
* ğŸ” **Vector Search** using embeddings
* ğŸ§  **LLMâ€‘based Answer Generation** grounded only in retrieved context
* ğŸ§¾ **Source Attribution** for every answer
* ğŸ›‘ **No Hallucination Guarantee** â€“ answers are refused if data is missing

---

## ğŸ—ï¸ System Architecture

```
User Question
     â†“
Query Embedding
     â†“
Vector Database (Pinecone)
     â†“
Topâ€‘K Relevant Chunks
     â†“
LLM (Contextâ€‘Restricted)
     â†“
Final Answer + Sources
```

> âš ï¸ The LLM is **not allowed to answer using its own knowledge**.

---

## ğŸ“ Project Structure

```
backend/
â”‚â”€â”€ main.py                 # FastAPI app
â”‚
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ pdf_ingest.py       # PDF ingestion
â”‚   â”œâ”€â”€ csv_ingest.py       # CSV ingestion
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ query.py            # Retrieval + answer pipeline
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ embeddings.py       # Text â†’ vector embeddings
â”‚   â”œâ”€â”€ pinecone_client.py  # Vector DB connection
â”‚
â”œâ”€â”€ llm.py                  # LLM wrapper (Groq)
â””â”€â”€ venv/
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Environment Variables

```bash
export GROQ_API_KEY=your_key_here
export PINECONE_API_KEY=your_key_here
```

---

## â–¶ï¸ Run the Server

```bash
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

Health check:

```bash
GET http://localhost:8001/health
```

---

## ğŸ“¥ Data Ingestion

### ğŸ“„ PDF Ingestion

```bash
curl -X POST "http://localhost:8001/ingest/pdf?path=/absolute/path/to/file.pdf"
```

### ğŸ“Š CSV Ingestion

```bash
curl -X POST "http://localhost:8001/ingest/csv?path=/absolute/path/to/file.csv"
```

What CSV ingestion does:

* Columnâ€‘level semantic understanding
* Rowâ€‘level data grounding
* Metadata tagging (`type = column | row`)

---

## â“ Querying the System

```bash
curl -X POST http://localhost:8001/query \
-H "Content-Type: application/json" \
-d '{
  "question": "Which coffee products generate the highest revenue?",
  "top_k": 5
}'
```

### Example Response

```json
{
  "question": "Which coffee products generate the highest revenue?",
  "answer": "Espresso generates the highest revenue.",
  "sources": ["./sales.csv"]
}
```

---

## ğŸ›‘ Hallucination Control

If the answer is **not present in the ingested documents**, the system responds with:

> "The answer is not available in the provided data."

This makes OmniRAG suitable for:

* Enterprise analytics
* Compliance systems
* Knowledge bases
* Academic projects

---

## ğŸ“ Academic Use (Finalâ€‘Year Project Ready)

This project demonstrates:

* Information Retrieval
* Semantic Search
* Embeddings
* Vector Databases
* LLM grounding
* Responsible AI (No hallucinations)

---

## ğŸ”® Future Enhancements (Optional)

* UI dashboard
* Authentication
* Chunk scoring threshold tuning
* Multiâ€‘file querying
* Tableâ€‘aware reasoning

---

## ğŸ‘¨â€ğŸ’» Author

**Ajay Vinayak**
Finalâ€‘Year B.Tech â€“ Computer Science (Big Data Analytics)

---

âœ… This project is intentionally scoped to remain **robust, explainable, and interviewâ€‘safe**.
# Omnirag-Multimodal-RAG
A production-ready multimodal RAG system that ingests documents and datasets, stores embeddings in Pinecone, and generates grounded answers using Grok.
